{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d302c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216acfd",
   "metadata": {},
   "source": [
    "Types of **SHAP** techniques (not really **model-agnostic** anymore but boosts performance of specific model):\n",
    "- **KernelSHAP**\n",
    "- **TreeSHAP**\n",
    "- **DeepSHAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bbef7",
   "metadata": {},
   "source": [
    "# SHAP\n",
    "**SHapley Additive exPlanations** <br>\n",
    "SHAP comes from **Cooperative Game Theory**<br> <br>\n",
    "- Imagine a game with $x$ players, all of them forming a **coalition**\n",
    "- When the game is over the gain a certain payout compared with their contributions\n",
    "- What is a fair distribution? $\\rightarrow$ **Shapley values**\n",
    "![alt text](images/shap_ex.png) <br>\n",
    "- Players $\\rightarrow$ **Features**\n",
    "- Payout $\\rightarrow$ **Model Prediction**\n",
    "- Game $\\rightarrow$ **ML Blackbox model** <br> <br>\n",
    "**The Main idea** : $\\rightarrow$ How the **prediction** (payout) would perform in different **Coalition** variations. (with or without some players) (every possible subset), then \n",
    "1. We calculate each players contribution for each subset (**coalition**)\n",
    "2. **Averaging** over all this contributions that gives us $\\rightarrow$ **Marginal Contribution** of a player to the team <br> <br>\n",
    "---\n",
    "### The paper (A Unified Approach to Interpreting Model Predictions)\n",
    "The Explainable AI technique **SHAP** simply makes use of these **shapley values** <br>\n",
    "- In general, it is a **local explanation technique** \n",
    "- **AIM:** to explain individual predicitons of black box models\n",
    "- However, it is possible to get **global explanations** through aggregating over these individual predictions<br><br>\n",
    "---\n",
    "### Calculating shapley values\n",
    "$\\phi_i \\rightarrow$ Shapley value for feature $i$ **(e.g.: {Age})**<br>\n",
    "$f \\rightarrow$ Black Box Model<br>\n",
    "$x \\rightarrow$ Input data point<br>\n",
    "$z' \\rightarrow$ Subset **(e.g.: {Age, BMI})**<br>\n",
    "$x' \\rightarrow$ Simplified data input<br>\n",
    "Using a **mapping function** we transform $x \\rightarrow x'$ <br>\n",
    "$z'\\subseteq x' \\rightarrow$ Iterate over all possible combinations of features<br>\n",
    "$f_x(z') \\rightarrow$ Black Box Model output for our input **with** the feature(s) we are interested in **(e.g.:{Age, BMI})**<br>\n",
    "$f_x(z' \\backslash i) \\rightarrow$ Black Box Model output for our input **without** the feature(s) we are interested in **(e.g.:{BMI})**<br>\n",
    "$[f_x(z') - f_x(z' \\backslash i)] \\rightarrow$ Tells us how feature $i$ contributed to that subset. Also called the **marginal value**<br>\n",
    "$M \\rightarrow$ Total number of features<br>\n",
    "$\\frac{|z'|!(M - |z'| - 1)!}{M!} \\rightarrow$ Weighting according to how many players are in that correlation<br><br>\n",
    "$$\\phi_i(f,x) = \\sum_{z'\\subseteq x'} \\frac{|z'|!(M - |z'| - 1)!}{M!} [f_x(z') - f_x(z' \\backslash i)]$$\n",
    "<br>\n",
    "- Intuition: The contribution of adding the feature $i$ should be weighted more if already many features are included in that subset\n",
    "<br>\n",
    "---\n",
    "### How do we exclude a feature from a ML model?\n",
    "![alt text](images/shap_ex2.png)\n",
    "- For the features we want to exclude we just **input random values from the train data set** and do this for all subsets\n",
    "- A random feature has usually no predictive power\n",
    "![alt text](images/shap_ex3.png)\n",
    "<br>\n",
    "---\n",
    "### Complexity of calculating Shapley Values\n",
    "- Where $n$ is the number of features <br>\n",
    "$$2^n = \\text{total number of subsets of a set}$$\n",
    "- For $10$ features:\n",
    "$$2^{10} = 1024$$ <br>\n",
    "> The basic idea is (as presented in the SHAP paper) that we can **simply approximate** the shapley values instead of calculating all possible combinations\n",
    ">> **KernelSHAP** for instance samples feature subsets and fits a linear regression model based on these samples. <br>\n",
    ">> The Variables in this Linear Regression model are simply: if a feature is present or absent. <br>\n",
    ">> After the training the coefficients $\\beta_n$ can be interpreted as approximated **shapley values**. <br>\n",
    ">> $$Y=x_1\\beta_1+x_2\\beta_2+...$$ <br>\n",
    ">> So we weight our samples based on how much information they contain\n",
    "                                                          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
