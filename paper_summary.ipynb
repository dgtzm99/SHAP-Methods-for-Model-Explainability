{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484fb648",
   "metadata": {},
   "source": [
    "### Target:\n",
    "> implement and test on alternative settings, compare conclusions to those of this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce03d0",
   "metadata": {},
   "source": [
    "# A Unified Approach to Interpreting Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a8c91",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "- We currently struggle to interpret complex ML models $\\rightarrow$ SHAP values\n",
    "- We need a nice balance between <i>accuracy<i> and <i>interpretability<i>. <br>\n",
    "> **SHAP values** gives each feature an Importance value for a given prediction (locally, from one example)\n",
    "<br><br>\n",
    "- Contents:\n",
    "    - Section 2: Introduce **Explanation model**<br>\n",
    "    - Section 3: Theorem: **Shapley values** are the only set of values that satisfy the properties of **Additive Feature Attribution Methods**<br>\n",
    "    - Section 4: SHAP values and methods that approximate it (DeepSHAP, ...)<br>\n",
    "    - Section 5: How SHAP is better than other existing methods\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ba080",
   "metadata": {},
   "source": [
    "## 2. Additive Feature Attribution Methods\n",
    "> **Explanation model** makes us view any explanation of a modelÂ´s prediction\n",
    "> - Additive Feature Attribution methods have an **explanation model** that is a linear function of binary values.\n",
    ">> $$g(z')= \\phi_0 + \\sum^{M}_{i=1} \\phi_i z'_i$$<br>\n",
    ">> $M \\rightarrow$ num of simplified input features<br>\n",
    ">> $\\phi_i \\in R \\rightarrow$ effect to each feature $i$<br>\n",
    ">> $z'\\in {0,1}^{M}$\n",
    "<br><br>\n",
    "---\n",
    "### AFAM #1 LIME\n",
    "- Model Agnostic (can be used for any ML model)\n",
    "- LIME fits a linear interpretable model in such area which is often called **surrogate** as well. Creating a local approximation.\n",
    "- **Math used in LIME:** <br>\n",
    "$x$ - input data point <br>\n",
    "$f$ - complex model <br>\n",
    "$g$ - Simple interpretable model (**surrogate**) <br>\n",
    "$G$ - Family of interpretable models(linear reg and its variants) <br>\n",
    "$\\pi$ - Defines local neighbourhood of $x$ data point, with some sort of **proximity measure**<br>\n",
    "$\\mathcal{L}(f, g, \\pi_x)$ - we look for an approximation of model $f$ by the simple model $g$ in the neighbourhood of our datapoint x $(\\pi_x)$ <br>\n",
    "$\\Omega(g)$ - regularize (simplify) the complexity of our simple **surrogate model** $(g)$ <br>\n",
    "$$ \\xi(x) = argmin_{g \\in G} \\mathcal{L}(f, g, \\pi_x) + \\Omega(g) $$ <br>\n",
    "- We look for a simple model $g$ that looks for the closest approximation of model $f$, and additionally stay as simple (minimize the complexity) as possible ($\\Omega(g)$)<br>\n",
    "---\n",
    "### AFAM #2 DeepLIFT\n",
    "- I havent covered it\n",
    "---\n",
    "### AFAM #3 Layer-wise Relevance Propagation\n",
    "- Not fully covered in paper\n",
    "- Equivalent to **DeepLIFT**\n",
    "---\n",
    "### AFAM #4 Classic Shapley Value Estimation\n",
    "- Shapley Regression Values\n",
    "- Shapley Sampling Values\n",
    "- Quantitative input influence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99614c15",
   "metadata": {},
   "source": [
    "## 3. AFAM Properties\n",
    "1. Local Accuracy\n",
    "    - Local accuarcy requires the explanation model $g(x')$ to at least match the output of $f$ for the simplified input $x'$ which corresponds to original input $x$. That is when $x=h_x(x')$<br>\n",
    "    $$f(x)=g(x')=\\phi_0 + \\sum^{M}_{i=1} \\phi_i x'_i$$<br>\n",
    "2. Missingness\n",
    "    - Missingness constrains features where $x_i=0$ to have no attributed impact\n",
    "    $$x'_i=0 \\rightarrow \\phi_i=0$$<br>\n",
    "3. Consistency\n",
    "    - If a model changes so that some simplified input's contribution increases or stays the same regardless of the other input, that input's attribution should not decrease\n",
    "    - (equation)<br>\n",
    "\n",
    "#### Conclusion\n",
    "- Methods not based on **Shapley values** violate local accuracy and/or consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1745cfa",
   "metadata": {},
   "source": [
    "## 4. About SHAP values\n",
    "> **SHAP** simply makes usse of these **Shappley Values**<br>\n",
    "### Math of Shapley values\n",
    "$\\phi_i \\rightarrow$ Shapley value for feature $i$ **(e.g.: {Age})**<br>\n",
    "$f \\rightarrow$ Black Box Model<br>\n",
    "$x \\rightarrow$ Input data point<br>\n",
    "$z' \\rightarrow$ Subset **(e.g.: {Age, BMI})**<br>\n",
    "$x' \\rightarrow$ Simplified data input<br>\n",
    "Using a **mapping function** we transform $x \\rightarrow x'$ <br>\n",
    "$z'\\subseteq x' \\rightarrow$ Iterate over all possible combinations of features<br>\n",
    "$f_x(z') \\rightarrow$ Black Box Model output for our input **with** the feature(s) we are interested in **(e.g.:{Age, BMI})**<br>\n",
    "$f_x(z' \\backslash i) \\rightarrow$ Black Box Model output for our input **without** the feature(s) we are interested in **(e.g.:{BMI})**<br>\n",
    "$[f_x(z') - f_x(z' \\backslash i)] \\rightarrow$ Tells us how feature $i$ contributed to that subset. Also called the **marginal value**<br>\n",
    "$M \\rightarrow$ Total number of features<br>\n",
    "$\\frac{|z'|!(M - |z'| - 1)!}{M!} \\rightarrow$ Weighting according to how many players are in that correlation<br><br>\n",
    "$$\\phi_i(f,x) = \\sum_{z'\\subseteq x'} \\frac{|z'|!(M - |z'| - 1)!}{M!} [f_x(z') - f_x(z' \\backslash i)]$$\n",
    "<br>\n",
    "- Intuition: The contribution of adding the feature $i$ should be weighted more if already many features are included in that subset\n",
    "<br>\n",
    "\n",
    "### Complexity of calculating Shapley Values\n",
    "- Where $n$ is the number of features <br>\n",
    "$$2^n = \\text{total number of subsets of a set}$$\n",
    "- For $10$ features:\n",
    "$$2^{10} = 1024$$ <br>\n",
    "- **However**, by combining current AFAM, we can approximate them <br>\n",
    "\n",
    "### Aproximation methods of SHAP\n",
    "\n",
    "#### 1. Shapley Sampling Values (Model-Agnostic)\n",
    "#### 2. Quantitative Input Influence (Model-Agnostic)\n",
    "#### 3. KernelSHAP (Model-Agnostic)\n",
    "> **KernelSHAP** (Linear LIME + Shapley values)\n",
    "#### 4. Linear SHAP (Model-Specific)\n",
    "#### 5. Low-Order SHAP (Model-Specific)\n",
    "#### 6. MaxSHAP (Model-Specific)\n",
    "#### 7. DeepSHAP (Model-Specific)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339aed7",
   "metadata": {},
   "source": [
    "## 5. Tests on Paper\n",
    "\n",
    "#### 1. Human intuition consistency with SHAP (A,B)\n",
    "#### 2. MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e76d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
